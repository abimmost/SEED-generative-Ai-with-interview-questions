{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4aPh3mdqDssFb/yjCsuUo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abimmost/SEED-generative-Ai-with-interview-questions/blob/full-contribute/exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv\n",
        "!virtualenv myenv\n",
        "!./myenv/bin/pip install fastapi transformers typing pydantic uvicorn pyngrok diffusers langchain_community"
      ],
      "metadata": {
        "id": "H3BNnAn05Zk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# 1. Define the path to the site-packages directory\n",
        "# You might need to change 'python3.10' based on the Colab runtime version\n",
        "env_path = './myenv/lib/'\n",
        "python_version = f\"python{sys.version_info.major}.{sys.version_info.minor}\"\n",
        "site_packages_path = os.path.join(env_path, python_version, 'site-packages')\n",
        "\n",
        "# 2. Add the path to Python's system path\n",
        "if site_packages_path not in sys.path:\n",
        "    sys.path.append(site_packages_path)\n",
        "    print(f\"Added {site_packages_path} to sys.path.\")\n",
        "else:\n",
        "    print(f\"{site_packages_path} is already in sys.path.\")"
      ],
      "metadata": {
        "id": "SeNMMWnI892Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ALL EXERCISES**"
      ],
      "metadata": {
        "id": "C22Z8RUQFFZb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiF7fWrQ8NuO"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from transformers import pipeline\n",
        "from typing import Optional\n",
        "\n",
        "# @title\n",
        "app = FastAPI()\n",
        "\n",
        "## EXERCISE 1: Hello LLM Endpoint\n",
        "\n",
        "@app.post(\"/hello-llm\")\n",
        "def hello(text: str):\n",
        "    hello_pipeline = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "    greeting = hello_pipeline(text)\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"Greetings\": greeting\n",
        "    }\n",
        "\n",
        "## EXERCISE 2: TEXT SUMMARIZER\n",
        "\n",
        "@app.post(\"/summarize\")\n",
        "def summary(long_text):\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    summarized = summarizer(\n",
        "        long_text,\n",
        "        max_length=50,\n",
        "        min_length=15,\n",
        "        do_sample=False\n",
        "    )\n",
        "    return {\n",
        "        \"Long Text\": long_text,\n",
        "        \"Summary\": summarized\n",
        "    }\n",
        "\n",
        "## EXERCISE 3: SENTIMENT ANALYSIS\n",
        "\n",
        "@app.post(\"/sentiment\")\n",
        "def sentiment(text: str):\n",
        "    sentiment_pipeline = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    senti = sentiment_pipeline(text)\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"sentiment\": senti\n",
        "    }\n",
        "\n",
        "## EXERCISE 4: MULTIMODAL IMAGE CAPTIONING\n",
        "# By uploading image\n",
        "@app.post(\"/caption-image\")\n",
        "async def run_file(file:UploadFile=File(...)):\n",
        "  from PIL import Image\n",
        "  import io\n",
        "\n",
        "  caption_pipeline = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "  image_bytes = await file.read()\n",
        "  image = Image.open(io.BytesIO(image_bytes))\n",
        "  captioned_image = caption_pipeline(image)\n",
        "  return {\n",
        "      \"file\": file.filename,\n",
        "      \"caption\": captioned_image\n",
        "    }\n",
        "\n",
        "# By sending image link\n",
        "@app.post(\"/caption-image-link\")\n",
        "def caption_image_link(image_link: str):\n",
        "  caption_pipeline = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "  captioned_image = caption_pipeline(image_link)\n",
        "  return {\n",
        "    \"file\": image_link,\n",
        "    \"caption\": captioned_image\n",
        "  }\n",
        "\n",
        "## EXERCISE 10: DIFFUSION MODEL - IMAGE GENERATION\n",
        "\n",
        "@app.post(\"/image-generation\")\n",
        "def image_gen(prompt: str):\n",
        "  from diffusers import DiffusionPipeline\n",
        "\n",
        "  pipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n",
        "  image = pipe(prompt).images[0]\n",
        "  return {\n",
        "    \"prompt\": prompt,\n",
        "    \"image\": image\n",
        "  }\n",
        "\n",
        "## EXERCISE 8: CHAIN MULTIPLE TOOLS WITH LANGCHAIN\n",
        "\n",
        "@app.post(\"/chain-tools\")\n",
        "def chain_tools(question: str):\n",
        "  from langchain_community.document_loaders import WikipediaLoader\n",
        "\n",
        "  wiki_loader = WikipediaLoader(query=question, lang=\"en\")\n",
        "  summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "  summarized = summarizer(\n",
        "      wiki_loader,\n",
        "      max_length=500,\n",
        "      min_length=100,\n",
        "      do_sample=False\n",
        "  )\n",
        "  sentiment_pipeline = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "  sentiment = sentiment_pipeline(summarized)\n",
        "\n",
        "  return {\n",
        "    \"question\": question,\n",
        "    \"answer\": summarized,\n",
        "    \"sentiment\": sentiment\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gUXmmwM1n10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SERVING FASTAPI**"
      ],
      "metadata": {
        "id": "E6OEIZh-FTBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "auth_token = \"33EuUmVjTDuTEVQeCW6lBNFcGQZ_4ZQGynoNCAXRdgFkKSaNi\"\n",
        "\n",
        "# Set the authtoken\n",
        "ngrok.set_auth_token(auth_token)\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops (required for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 1. Start the ngrok tunnel on the port where Uvicorn will run\n",
        "NGROK_TUNNEL = ngrok.connect(49000)\n",
        "print(\"Public URL:\", NGROK_TUNNEL.public_url+\"/docs\")\n",
        "\n",
        "# 2. Run Uvicorn\n",
        "# The loop will block the notebook, but the URL will be active.\n",
        "# Use reload=False in a Colab environment.\n",
        "uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=49000, log_level=\"info\")"
      ],
      "metadata": {
        "id": "5obwjfjZ18D3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}