{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+xe+KbKakqmor0XtLycoJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abimmost/SEED-generative-Ai-with-interview-questions/blob/full-contribute/exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi transformers typing pydantic uvicorn pyngrok"
      ],
      "metadata": {
        "id": "SeNMMWnI892Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ALL** **EXERCISES**"
      ],
      "metadata": {
        "id": "C22Z8RUQFFZb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiF7fWrQ8NuO"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from transformers import pipeline\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# @title\n",
        "app = FastAPI()\n",
        "\n",
        "## Hello LLM Endpoint\n",
        "\n",
        "class Hello(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post(\"/hello-llm\")\n",
        "def hello(request:Hello):\n",
        "    hello_pipeline = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "    greeting = hello_pipeline(request.text)\n",
        "    return {\n",
        "        \"text\": request.text,\n",
        "        \"Greetings\": greeting\n",
        "    }\n",
        "\n",
        "## TEXT SUMMARIZER\n",
        "\n",
        "class summary(BaseModel):\n",
        "    long_text: str\n",
        "\n",
        "@app.post(\"/summarize\")\n",
        "def summary(request:summary):\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    summarized = summarizer(\n",
        "        request.long_text,\n",
        "        max_length=50,\n",
        "        min_length=15,\n",
        "        do_sample=False\n",
        "    )\n",
        "    return {\n",
        "        \"Long Text\": request.long_text,\n",
        "        \"Summary\": summarized\n",
        "    }\n",
        "\n",
        "## SENTIMENT ANALYSIS\n",
        "\n",
        "class Sentiment(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post(\"/sentiment\")\n",
        "def senti(request:Sentiment):\n",
        "    sentiment_pipeline = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    senti = sentiment_pipeline(request.text)\n",
        "\n",
        "    return {\n",
        "        \"text\": request.text,\n",
        "        \"sentiment\": senti\n",
        "    }\n",
        "\n",
        "## MULTIMODAL IMAGE CAPTIONING\n",
        "\n",
        "@app.post(\"/caption-image\")\n",
        "def caption_image(file:UploadFile=File(...)):\n",
        "    caption_pipeline = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "    captioned_image = caption_pipeline(file)\n",
        "\n",
        "    return {\n",
        "        \"file\": file.filename,\n",
        "        \"caption\": captioned_image\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SERVING FASTAPI**"
      ],
      "metadata": {
        "id": "E6OEIZh-FTBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "auth_token = \"33EuUmVjTDuTEVQeCW6lBNFcGQZ_4ZQGynoNCAXRdgFkKSaNi\"\n",
        "\n",
        "# Set the authtoken\n",
        "ngrok.set_auth_token(auth_token)\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops (required for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 1. Start the ngrok tunnel on the port where Uvicorn will run\n",
        "NGROK_TUNNEL = ngrok.connect(8000)\n",
        "print(\"Public URL:\", NGROK_TUNNEL.public_url)\n",
        "\n",
        "# # 2. Run Uvicorn\n",
        "# # The loop will block the notebook, but the URL will be active.\n",
        "# # Use reload=False in a Colab environment.\n",
        "# uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Md3c3O9aWaTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}